from random import randint,random ,uniform

class Data:
    data_dict= {}
    def __init__(self,limit):
        self.limit = limit
    '''creates data but beware that the limit may not be the same as the size of the dictionary''' 
    def create_data(self):
        for i in range(self.limit):
            num1 = randint(0,100)
            num2 = randint(0,100)
            self.data_dict[(num1,num2)] = num1+num2


''' you compare the error with every test in the data set and find weights that minimise the error'''
class Neural:
    def __init__(self,data):
        self.x1 = uniform(-1,1) 
        self.x2 = uniform(-1,1) 
        self.y1 = uniform(-1,1) 
        self.y2 = uniform(-1,1) 
        self.z1 = uniform(-1,1) 
        self.z2 = uniform(-1,1) 
        self.data=data
    
    def relu(self,number):
        return max(0,number)
        
    '''weighted summation with activation function to compute output '''
    def compute_output(self,num1,num2):
        hidden_layer_input1 = self.relu(num1 * self.x1) + (num2 * self.y1)
        hidden_layer_input2 = self.relu(num1 * self.x2) + (num2 * self.y2)
        return (hidden_layer_input1*self.z1) + (hidden_layer_input2 * self.z2)
            
    '''error between the actual output with the output generated by the algorithm '''
    def compare_ouput(self,data):
        '''actually,better to find error between all tests. add all the errors up'''
        error = 0
        for key in data.data_dict:
            error += abs(data.data_dict[key] - self.compute_output(key[0],key[1])) **2
        return error/len(data.data_dict)
       # return abs(actual - self.compute_output(num1,num2))

    '''TODO function that changes the weight depending on the errors using gradient descent'''
    '''first make it random'''
    '''next perhaps change weights for each test and average out the adjustments for each weight'''
    def random_back_propagation(self):
        error = 100000
        while error>0.1:
            self.x1 = random() 
            self.x2 = random()
            self.y1 = random()
            self.y2 = random()
            self.z1 = random()
            self.z2 = random()
            error = self.compare_ouput(self.data)
            print(error)
        print(self.compute_output(140,15))
            


data = Data(200)
data.create_data()
neural = Neural(data)
neural.random_back_propagation()
#print(data.data_array)
print(uniform(-1,1))



        