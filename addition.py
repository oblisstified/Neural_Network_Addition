from random import randint, random, uniform
import numpy as np


class Data:
    data_dict = {}

    def __init__(self, limit):
        self.limit = limit

    """creates data but beware that the limit may not be the same as the size of the dictionary"""

    def create_data(self):
        for i in range(self.limit):
            num1 = randint(0, 100)
            num2 = randint(0, 100)
            self.data_dict[(num1, num2)] = num1 + num2


""" you compare the error with every test in the data set and find weights that minimise the error"""


class Neural:
    def __init__(self, data):
        self.x1 = 0.2 * uniform(-1, 1) 
        self.x2 = 0.2 * uniform(-1, 1) 
        self.y1 = 0.2 * uniform(-1, 1) 
        self.y2 = 0.2 * uniform(-1, 1) 
        self.x3 = 0.2 * uniform(-1, 1)
        self.y3 = 0.2 * uniform(-1, 1) 
        self.z1 = 0.2 * uniform(-1, 1)
        self.z2 = 0.2 * uniform(-1, 1) 
        self.z3 = 0.2 * uniform(-1, 1) 
        self.data = data

    def relu(self, number):
        return max(0, number)

    def sigmoid(self, number):
        return 1 / (1 + np.exp(-number))

    """weighted summation with activation function to compute output """

    def compute_output(self, num1, num2):
        num1, num2 = num1 / 100, num2 / 100
        hidden_layer_input1 = self.sigmoid((num1 * self.x1) + (num2 * self.y1))
        hidden_layer_input2 = self.sigmoid((num1 * self.x2) + (num2 * self.y2))
        hidden_layer_input3 = self.sigmoid((num1 * self.x3) + (num2 * self.y3))
        return 100 * ((hidden_layer_input1 * self.z1) + (hidden_layer_input2 * self.z2) + (hidden_layer_input3 * self.z3))

    """mean swaured error error between the actual output with the output generated by the algorithm """

    def compare_ouput(self, data):
        """actually,better to find error between all tests. add all the errors up"""
        error = 0
        for key in data.data_dict:
            error += abs(data.data_dict[key] - self.compute_output(key[0], key[1])) ** 2
        return error / len(data.data_dict)

    # return abs(actual - self.compute_output(num1,num2))

    """TODO function that changes the weight depending on the errors using gradient descent"""
    """first make it random"""
    """next perhaps change weights for each test and average out the adjustments for each weight"""

    def random_back_propagation(self):
        error = 100000
        while error > 0.1:
            self.x1 = random()
            self.x2 = random()
            self.y1 = random()
            self.y2 = random()
            self.z1 = random()
            self.z2 = random()
            error = self.compare_ouput(self.data)
            print(error)
        print(self.compute_output(140, 15))

        """learning rate is the amount the weights are updated during training"""

    def back_propagation(self, learning_rate):
        for epoch in range(100000):
            errors = []
            for key in self.data.data_dict:

                num1, num2 = key
                target = self.data.data_dict[key]

                # Rescaling everything
                num1, num2 = num1 / 100, num2 / 100
                target = target / 100

                hidden_layer1_output = self.sigmoid((num1 * self.x1) + (num2 * self.y1))
                hidden_layer2_output = self.sigmoid((num1 * self.x2) + (num2 * self.y2))
                hidden_layer3_output = self.sigmoid((num1 * self.x3) + (num2 * self.y3))
                

                output = (hidden_layer1_output * self.z1) + (hidden_layer2_output * self.z2) + (hidden_layer3_output * self.z3)

                # loss
                error = (target - output) ** 2
                # check if you are happpy with the error

                # derivative of error with respect to output
                # d(error)/d(output) = -2 * (target - output)

                # derivative of error with respect to x1
                # d(error)/d(x1) = d(error)/d(output) * d(output)/d(x1)
                #                                                      /-> hidden_layer1_output
                # d(error)/d(x1) = d(error)/d(output) * ( d(output)/d(hl1) * d(hl1)/d(x1) )

                #   derivative of output with respect to hl1
                #   d(output)/d(hl1) = z1

                #   derivative of hl1 with respect to x1
                #   d(hl1)/d(x1) = hidden_layer1_output * (1 - hidden_layer1_output) * num1

                # d(error)/d(x1) = -2 * (target - output) * z1 * hidden_layer1_output * (1 - hidden_layer1_output) * num1

                self.x1 -= learning_rate * -2 * (target - output) * self.z1 * hidden_layer1_output * (1 - hidden_layer1_output) * num1
                self.y1 -= learning_rate * -2 * (target - output) * self.z1 * hidden_layer1_output * (1 - hidden_layer1_output) * num2
                self.x2 -= learning_rate * -2 * (target - output) * self.z2 * hidden_layer2_output * (1 - hidden_layer2_output) * num1
                self.y2 -= learning_rate * -2 * (target - output) * self.z2 * hidden_layer2_output * (1 - hidden_layer2_output) * num2
                self.x3 -= learning_rate * -2 * (target - output) * self.z3 * hidden_layer3_output * (1 - hidden_layer3_output) * num1
                self.y3 -= learning_rate * -2 * (target - output) * self.z3 * hidden_layer3_output * (1 - hidden_layer3_output) * num2
                # derivative of error with respect to z1
                # d(error)/d(z1) = d(error)/d(output) * d(output)/d(z1)

                #   derivative of output with respect to z1
                #   d(output)/d(z1) = hidden_layer1_output

                # d(error)/d(z1) = -2(target - output) * hidden_layer1_output
                self.z1 -= learning_rate * -2 * (target - output) * hidden_layer1_output
                self.z2 -= learning_rate * -2 * (target - output) * hidden_layer2_output
                self.z3 -= learning_rate * -2 * (target - output) * hidden_layer3_output

                # print(self.x1, self.x2, self.y1, self.y2, self.z1, self.z2)
                errors.append(error)
            print(f"Mean error: {np.mean(errors)}")


data = Data(2000)
data.create_data()

neural = Neural(data)
neural.back_propagation(0.08)
print("#################################PREDICTIONS############################################")
print(f"15 + 7 = {neural.compute_output(15, 7)}")
print(f"3 + 2 = {neural.compute_output(3, 2)}")
print(f"50 + 70 = {neural.compute_output(50, 70)}")
print(f"100 + 250 = {neural.compute_output(100, 250)}")

    